{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ccad3b4-f047-4baf-be9e-f737b510fa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autoawq 0.2.7.post3 requires datasets>=2.20, but you have datasets 2.19.1 which is incompatible.\n",
      "axolotl 0.9.2 requires datasets==3.5.1, but you have datasets 2.19.1 which is incompatible.\n",
      "axolotl 0.9.2 requires evaluate==0.4.1, but you have evaluate 0.4.3 which is incompatible.\n",
      "axolotl 0.9.2 requires hf_xet==1.1.0, but you have hf-xet 1.1.3 which is incompatible.\n",
      "axolotl 0.9.2 requires huggingface_hub==0.31.0, but you have huggingface-hub 0.33.0 which is incompatible.\n",
      "trl 0.17.0 requires datasets>=3.0.0, but you have datasets 2.19.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49e30e2b-98ca-4265-92a4-6cb6030501b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: evaluate 0.4.3\n",
      "Uninstalling evaluate-0.4.3:\n",
      "  Successfully uninstalled evaluate-0.4.3\n",
      "Found existing installation: datasets 3.5.1\n",
      "Uninstalling datasets-3.5.1:\n",
      "  Successfully uninstalled datasets-3.5.1\n",
      "Found existing installation: huggingface-hub 0.31.0\n",
      "Uninstalling huggingface-hub-0.31.0:\n",
      "  Successfully uninstalled huggingface-hub-0.31.0\n",
      "Found existing installation: axolotl 0.9.2\n",
      "Uninstalling axolotl-0.9.2:\n",
      "  Successfully uninstalled axolotl-0.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall -y evaluate datasets huggingface_hub axolotl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdecf04d-3fda-41c0-8122-6f49b32822c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"huggingface_hub==0.33.0\" \"datasets==3.5.1\" \"evaluate==0.4.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d3983d8-6778-4ccd-b8c1-152fc35c2140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bert_score) (2.6.0+cu124)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bert_score) (2.1.4)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bert_score) (4.51.3)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bert_score) (1.26.4)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bert_score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bert_score) (3.8.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bert_score) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.14.0)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.33.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.5.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->bert_score) (1.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib->bert_score) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib->bert_score) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib->bert_score) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib->bert_score) (3.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->bert_score) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->bert_score) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->bert_score) (2025.4.26)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: bert_score\n",
      "Successfully installed bert_score-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a5356d9-062c-4755-86cb-394413063ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libaio1 is already the newest version (0.3.112-5).\n",
      "build-essential is already the newest version (12.8ubuntu1.1).\n",
      "libstdc++6 is already the newest version (10.5.0-1ubuntu1~20.04).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 12 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install -y libaio1 libstdc++6 build-essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d24607d-cb90-4e72-b086-c986f5bebac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab51a52d-b253-4f9a-8293-eab017d2611b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b6c76cefd04fd0b6e742cc5d7b4021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2a50f4-faf8-45a6-b35c-74c5d4999ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_local_dataset(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c47e3413-1d19-48c2-bce2-1254fb6bf652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import evaluate\n",
    "\n",
    "def evaluate_predictions_from_file(pred_file):\n",
    "    with open(pred_file, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "    preds = [d[\"model_output\"] for d in data]\n",
    "    refs = [d[\"expected_output\"] for d in data]\n",
    "\n",
    "    # ROUGE\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    rouge_score = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "\n",
    "    # BLEU\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    bleu_score = bleu.compute(predictions=preds, references=[[ref] for ref in refs])\n",
    "\n",
    "    # METEOR\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    meteor_score = meteor.compute(predictions=preds, references=refs)\n",
    "\n",
    "    # ChrF\n",
    "    chrf = evaluate.load(\"chrf\")\n",
    "    chrf_score = chrf.compute(predictions=preds, references=refs)\n",
    "\n",
    "    # BERTScore\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    bert_score = bertscore.compute(predictions=preds, references=refs, lang=\"en\")\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": round(rouge_score[\"rougeL\"] * 100, 2),\n",
    "        \"bleu\": round(bleu_score[\"bleu\"] * 100, 2),\n",
    "        \"meteor\": round(meteor_score[\"meteor\"] * 100, 2),\n",
    "        \"chrf\": round(chrf_score[\"score\"], 2),\n",
    "        \"bertscore_f1\": round(sum(bert_score[\"f1\"]) / len(bert_score[\"f1\"]) * 100, 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1e289bd-7551-46bb-ae2a-61165b95a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_inference(model_name_or_path, test_data):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        device_map=\"auto\",\n",
    "        load_in_8bit=True\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    for item in tqdm(test_data):\n",
    "        prompt = item[\"prompt\"]\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                do_sample=False\n",
    "            )\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        if generated_text.startswith(prompt):\n",
    "            generated_text = generated_text[len(prompt):].strip()\n",
    "        predictions.append(generated_text)\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "842ed2fe-ecff-4033-9255-a2ca69acfffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_model_evaluation(model_id, model_path, test_data, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    refs = [item[\"expected_output\"] for item in test_data]\n",
    "\n",
    "    print(f\"Running inference for: {model_id}\")\n",
    "    preds = run_model_inference(model_path, test_data)\n",
    "\n",
    "    # Save outputs\n",
    "    output_file = f\"{output_dir}/{model_id}_outputs.jsonl\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for item, pred in zip(test_data, preds):\n",
    "            f.write(json.dumps({\n",
    "                \"prompt\": item[\"prompt\"],\n",
    "                \"expected_output\": item[\"expected_output\"],\n",
    "                \"model_output\": pred\n",
    "            }) + \"\\n\")\n",
    "\n",
    "    print(f\"{model_id} inference completed. Output saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35a7248e-b03a-4cac-9226-64ec60de4e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(dataset, k):\n",
    "    return random.sample(dataset, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b06523f-b141-4878-bbbd-b7a55ae21cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_local_dataset(\"validation.jsonl\")\n",
    "sampled_data = sample_dataset(test_data, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "847e485f-2122-432a-b3ca-0819cc9b66b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference for: hermes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15f1482afc14d0ea66eed97dc06979c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/13.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4bc4d0d8f24eba9bb4336bd2b69640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b10422b1868497a88aeaf3da17a0ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.51M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e58464006824df7a73bb2a943725a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/739 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda888a6d4d940219778dd6f2c72340c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/566 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9266bf195f43aebb81998086150615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/621 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93084d975f864a699f7562245db721c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747fb825ae5d4bceae14b64705d12cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ee640dcf44437e8f4c9033922a299f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c04c4b5170478786d9d0d945ede93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb7a28a80254993b3a422a956117a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3dd2a2a68bb4b2ba085088d24ad9d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3639ade0ad9451bb19806282233d7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/136 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  1%|          | 1/100 [00:01<02:22,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  2%|▏         | 2/100 [00:11<10:20,  6.33s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  3%|▎         | 3/100 [00:11<05:49,  3.60s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  4%|▍         | 4/100 [00:27<13:31,  8.46s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  5%|▌         | 5/100 [00:38<14:44,  9.31s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  6%|▌         | 6/100 [00:45<13:15,  8.46s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  7%|▋         | 7/100 [01:17<25:09, 16.23s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  8%|▊         | 8/100 [01:17<17:08, 11.18s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  9%|▉         | 9/100 [01:23<14:37,  9.64s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 10%|█         | 10/100 [01:31<13:41,  9.13s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 11%|█         | 11/100 [01:44<14:54, 10.06s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 12%|█▏        | 12/100 [02:04<19:26, 13.26s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 13%|█▎        | 13/100 [02:11<16:29, 11.38s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 14%|█▍        | 14/100 [02:21<15:46, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 15%|█▌        | 15/100 [02:46<21:20, 15.07s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 16%|█▌        | 16/100 [02:55<18:37, 13.30s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 17%|█▋        | 17/100 [03:20<23:23, 16.90s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 18%|█▊        | 18/100 [03:21<16:17, 11.92s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 19%|█▉        | 19/100 [03:27<13:46, 10.21s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 20%|██        | 20/100 [03:38<13:55, 10.45s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 21%|██        | 21/100 [04:08<21:24, 16.26s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 22%|██▏       | 22/100 [04:17<18:31, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 23%|██▎       | 23/100 [04:24<15:18, 11.93s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 24%|██▍       | 24/100 [04:34<14:27, 11.42s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 25%|██▌       | 25/100 [05:00<19:41, 15.75s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 26%|██▌       | 26/100 [05:00<13:42, 11.12s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 27%|██▋       | 27/100 [05:01<09:36,  7.89s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 28%|██▊       | 28/100 [05:11<10:23,  8.66s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 29%|██▉       | 29/100 [05:16<08:54,  7.52s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 30%|███       | 30/100 [05:25<09:24,  8.06s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 31%|███       | 31/100 [05:35<09:50,  8.56s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 32%|███▏      | 32/100 [05:35<06:54,  6.09s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 33%|███▎      | 33/100 [05:47<08:49,  7.90s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 34%|███▍      | 34/100 [05:55<08:45,  7.96s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 35%|███▌      | 35/100 [06:12<11:22, 10.50s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 36%|███▌      | 36/100 [06:20<10:19,  9.68s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 37%|███▋      | 37/100 [06:28<09:40,  9.21s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 38%|███▊      | 38/100 [06:41<10:39, 10.32s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 39%|███▉      | 39/100 [06:48<09:38,  9.48s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 40%|████      | 40/100 [06:52<07:55,  7.93s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 41%|████      | 41/100 [07:00<07:34,  7.70s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 42%|████▏     | 42/100 [07:09<07:47,  8.07s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 43%|████▎     | 43/100 [07:17<07:52,  8.28s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 44%|████▍     | 44/100 [07:31<09:15,  9.92s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 45%|████▌     | 45/100 [07:48<11:00, 12.01s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 46%|████▌     | 46/100 [07:54<09:19, 10.36s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 47%|████▋     | 47/100 [08:03<08:34,  9.70s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 48%|████▊     | 48/100 [08:19<10:14, 11.82s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 49%|████▉     | 49/100 [08:20<07:06,  8.36s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 50%|█████     | 50/100 [08:29<07:14,  8.69s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 51%|█████     | 51/100 [08:45<08:50, 10.83s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 52%|█████▏    | 52/100 [09:12<12:35, 15.74s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 53%|█████▎    | 53/100 [09:19<10:12, 13.03s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 54%|█████▍    | 54/100 [09:19<07:04,  9.22s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 55%|█████▌    | 55/100 [09:31<07:30, 10.01s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 56%|█████▌    | 56/100 [09:58<11:07, 15.18s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 57%|█████▋    | 57/100 [10:05<09:00, 12.58s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 58%|█████▊    | 58/100 [10:05<06:14,  8.91s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 59%|█████▉    | 59/100 [10:14<06:07,  8.95s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 60%|██████    | 60/100 [10:15<04:14,  6.36s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 61%|██████    | 61/100 [10:21<04:04,  6.28s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 62%|██████▏   | 62/100 [10:28<04:12,  6.64s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 63%|██████▎   | 63/100 [10:34<03:51,  6.27s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 64%|██████▍   | 64/100 [10:41<03:54,  6.52s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 65%|██████▌   | 65/100 [10:55<05:14,  8.99s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 66%|██████▌   | 66/100 [11:02<04:40,  8.26s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 67%|██████▋   | 67/100 [11:08<04:07,  7.51s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 68%|██████▊   | 68/100 [11:08<02:52,  5.38s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 69%|██████▉   | 69/100 [11:26<04:39,  9.03s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 70%|███████   | 70/100 [11:42<05:37, 11.25s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 71%|███████   | 71/100 [11:50<05:01, 10.39s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 72%|███████▏  | 72/100 [11:51<03:26,  7.37s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 73%|███████▎  | 73/100 [11:58<03:16,  7.29s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 74%|███████▍  | 74/100 [12:07<03:24,  7.88s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 75%|███████▌  | 75/100 [12:16<03:25,  8.22s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 76%|███████▌  | 76/100 [12:31<04:06, 10.25s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 77%|███████▋  | 77/100 [12:38<03:29,  9.13s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 78%|███████▊  | 78/100 [12:49<03:35,  9.79s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 79%|███████▉  | 79/100 [13:00<03:33, 10.14s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 80%|████████  | 80/100 [13:00<02:24,  7.20s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 81%|████████  | 81/100 [13:07<02:14,  7.06s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 82%|████████▏ | 82/100 [13:17<02:22,  7.89s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 83%|████████▎ | 83/100 [13:24<02:08,  7.57s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 84%|████████▍ | 84/100 [13:30<01:52,  7.06s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 85%|████████▌ | 85/100 [13:39<01:54,  7.64s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 86%|████████▌ | 86/100 [13:39<01:16,  5.44s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 87%|████████▋ | 87/100 [13:39<00:50,  3.90s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 88%|████████▊ | 88/100 [13:46<00:58,  4.91s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 89%|████████▉ | 89/100 [13:47<00:38,  3.54s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 90%|█████████ | 90/100 [14:08<01:27,  8.70s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 91%|█████████ | 91/100 [14:08<00:55,  6.19s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 92%|█████████▏| 92/100 [14:18<00:58,  7.31s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 93%|█████████▎| 93/100 [14:39<01:20, 11.51s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 94%|█████████▍| 94/100 [14:50<01:07, 11.27s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 95%|█████████▌| 95/100 [14:59<00:53, 10.61s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 96%|█████████▌| 96/100 [15:11<00:44, 11.13s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 97%|█████████▋| 97/100 [15:18<00:29,  9.77s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 98%|█████████▊| 98/100 [15:18<00:13,  6.93s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 99%|█████████▉| 99/100 [15:34<00:09,  9.60s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "100%|██████████| 100/100 [15:45<00:00,  9.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hermes inference completed. Output saved to outputs/hermes_outputs.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_single_model_evaluation(\"hermes\", \"capstone-research/Hermes-2-Pro-Mistral-7B-Travel-ChatBot\", sampled_data, \"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "841468bd-e21c-40d6-9dfa-7eed70ad0cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference for: openchat\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883099ca4f154e239692a490fa57a8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cece9cc3f034442e83b6b54d44ec3434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317dad54a4c74124a79ed9ccd8bda039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.51M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0f98c5b8ba4194b2d1be2d88d23a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/148 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c607e35e1c846048e8beab32e02b2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae078b474184031aa74e4cacfde1923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/620 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586f84f282f24b29ac85098b95ea61b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76792f1bb06b4612b733d6457628fb40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb9f4c40a2447c2ab3ed208f5f33d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9a8774026145ee96a70374be90ef2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4e90e19674453ca1a12a19fc86b59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7297f4221e14c2b819446e2f674c82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1857027528f44d61a1596af26d37f236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 100/100 [45:06<00:00, 27.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openchat inference completed. Output saved to outputs/openchat_outputs.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_single_model_evaluation(\"openchat\", \"capstone-research/OpenChat-3.5-Travel-ChatBot\", sampled_data, \"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cdbf5f2-ee2d-466e-bb88-eaa97effbab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference for: tinyllama\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410b9937549e4a3db3c38da598662446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74d336b6df6437aa3be39ec8a68a27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f80c9cd611408bb17392464c8802d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.90M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513664103c2f478aa501b58d4e7964c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/104 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19454c51d0ce437e82157bb90b03b973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca52e51c9ce45cc8a38de90d9a86007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/712 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b9599d40004b21b956c1c7f3511fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fead402c8dd4313a5c629bda70a1d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/153 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [20:19<00:00, 12.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tinyllama inference completed. Output saved to outputs/tinyllama_outputs.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_single_model_evaluation(\"tinyllama\", \"capstone-research/TinyLlama-1.1B-Travel-ChatBot\", sampled_data, \"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c9319a-2207-4279-9ba5-a4f113040648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_final_summary(output_dir):\n",
    "    print(\"\\nFinal Evaluation Summary:\\n\")\n",
    "    summary = []\n",
    "    for file in os.listdir(output_dir):\n",
    "        if file.endswith(\"_outputs.jsonl\"):\n",
    "            model_id = file.replace(\"_outputs.jsonl\", \"\")\n",
    "            score = evaluate_predictions_from_file(os.path.join(output_dir, file))\n",
    "            summary.append({\"Model\": model_id, **score})\n",
    "\n",
    "    df = pd.DataFrame(summary).sort_values(by=\"bertscore_f1\", ascending=False).reset_index(drop=True)\n",
    "    print(df.to_markdown(index=False))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5576389-a384-488c-860a-14d6aa29e3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ~/.cache/huggingface/modules/evaluate_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a25069f5-f866-454b-84ae-a17d67070591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation Summary:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4de2958fda64b9488903293fa741764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420586ccf2414ff483af7c4d8753bec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b3807858aa4ab7b0b61d0c133c0108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f14bd662c34cb6924e0d0555825c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985b38e270de4c048d280e1a94e2a0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de5cda6624d44aba57e288be1cc3134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model     |   rougeL |   bleu |   meteor |   chrf |   bertscore_f1 |\n",
      "|:----------|---------:|-------:|---------:|-------:|---------------:|\n",
      "| tinyllama |    31.83 |  19.61 |    52.75 |  53.74 |          89.53 |\n",
      "| openchat  |    29.72 |  17.88 |    49.18 |  51    |          88.42 |\n",
      "| hermes    |    42.37 |  37.8  |    47.78 |  48.91 |          74.27 |\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>bleu</th>\n",
       "      <th>meteor</th>\n",
       "      <th>chrf</th>\n",
       "      <th>bertscore_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tinyllama</td>\n",
       "      <td>31.83</td>\n",
       "      <td>19.61</td>\n",
       "      <td>52.75</td>\n",
       "      <td>53.74</td>\n",
       "      <td>89.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>openchat</td>\n",
       "      <td>29.72</td>\n",
       "      <td>17.88</td>\n",
       "      <td>49.18</td>\n",
       "      <td>51.00</td>\n",
       "      <td>88.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hermes</td>\n",
       "      <td>42.37</td>\n",
       "      <td>37.80</td>\n",
       "      <td>47.78</td>\n",
       "      <td>48.91</td>\n",
       "      <td>74.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model  rougeL   bleu  meteor   chrf  bertscore_f1\n",
       "0  tinyllama   31.83  19.61   52.75  53.74         89.53\n",
       "1   openchat   29.72  17.88   49.18  51.00         88.42\n",
       "2     hermes   42.37  37.80   47.78  48.91         74.27"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_final_summary(\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a07f0e-52cc-42e3-b9ff-38c6076ebe70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
